# STATS ----------------------------------------------------------------------------------------------------------------
import pyodbc
import requests
from bs4 import BeautifulSoup
from bs4 import Comment
sql_connection = 'DRIVER={SQL Server Native Client 11.0};SERVER=localhost;DATABASE=BotTester;UID=sa;PWD=wa11paper'

def getfromtable(table, playoffs, player_id):
    def getstat(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            # print(row)
            season = row.th.get_text()
            age = getstat(row, "age")
            team_id = getstat(row, "team_id")
            pos = getstat(row, "pos")
            g = getstat(row, "g")
            gs = getstat(row, "gs")
            mp_per_g = getstat(row, "mp_per_g")
            fg_per_g = getstat(row, "fg_per_g")
            fga_per_g = getstat(row, "fga_per_g")
            fg_pct = getstat(row, "fg_pct")
            fg3_per_g = getstat(row, "fg3_per_g")
            fg3a_per_g = getstat(row, "fg3a_per_g")
            fg3_pct = getstat(row, "fg3_pct")
            fg2_per_g = getstat(row, "fg2_per_g")
            fg2a_per_g = getstat(row, "fg2a_per_g")
            fg2_pct = getstat(row, "fg2_pct")
            efg_pct = getstat(row, "efg_pct")
            ft_per_g = getstat(row, "ft_per_g")
            fta_per_g = getstat(row, "fta_per_g")
            ft_pct = getstat(row, "ft_pct")
            orb_per_g = getstat(row, "orb_per_g")
            drb_per_g = getstat(row, "drb_per_g")
            trb_per_g = getstat(row, "trb_per_g")
            ast_per_g = getstat(row, "ast_per_g")
            stl_per_g = getstat(row, "stl_per_g")
            blk_per_g = getstat(row, "blk_per_g")
            tov_per_g = getstat(row, "tov_per_g")
            pf_per_g = getstat(row, "pf_per_g")
            pts_per_g = getstat(row, "pts_per_g")

            # print(season, age, team_id, pos, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per_g, fg3a_per_g,
            #       fg3_pct, fg2_per_g, fg2a_per_g, fg2_pct, efg_pct, ft_per_g, fta_per_g, ft_pct, orb_per_g, drb_per_g,
            #       trb_per_g, ast_per_g, stl_per_g, blk_per_g, tov_per_g, pf_per_g, pts_per_g)

            # cursor.execute("insert into PlayerStats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed"
            #                ")"
            #                "values (?, ?, ?, ?, ?, ?, ?)",
            #                player_id, playoffs, season, age, team_id, pos, g)
            cursor.execute("insert into PlayerStats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed,"
                           " GamesStarted, MinPerGame, FgPerGame, FgaPerGame, FgPercent, Fg3PerGame, Fg3aPerGame, "
                           "Fg3Percent, Fg2PerGame, Fg2aPerGame, Fg2Percent, EfgPercent, FtPerGame, FtaPerGame, "
                           "FtPercent, OffRebound, DefRebound, Rebounds, Assists, Steals, Blocks, Turnovers, Fouls, "
                           "Points) "
                           "values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"
                           " ?, ?, ?)",
                           player_id, playoffs, season, age, team_id, pos, g, gs, mp_per_g, fg_per_g,
                           fga_per_g, fg_pct, fg3_per_g, fg3a_per_g, fg3_pct, fg2_per_g, fg2a_per_g, fg2_pct, efg_pct,
                           ft_per_g, fta_per_g, ft_pct, orb_per_g, drb_per_g, trb_per_g, ast_per_g, stl_per_g,
                           blk_per_g, tov_per_g, pf_per_g, pts_per_g)

    cursor.commit()

def scrape_data(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="per_game")
    getfromtable(table, 0, player_id)

    table = soup.find(id="playoffs_per_game")

    if table == None:
        return

    getfromtable(table, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_data(row.PlayerId, row.Link)

# PLAYERINFO -----------------------------------------------------------------------------------------------------------
def get_players(str):
    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    url = 'https://www.basketball-reference.com/players/' + str

    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="div_players")
    rows = table.find_all("tr")
    for row in rows:
        if row.th is not None and row.th['data-stat'] == 'player' \
                and row.th['scope'] == 'row' and row.th.get_text() != "":
            # print(row)
            name = row.th.get_text()
            playerid = row.th['data-append-csv']
            link = row.th.a['href']
            year_min = row.find(attrs={"data-stat": "year_min"}).get_text()
            year_max = row.find(attrs={"data-stat": "year_max"}).get_text()
            height = row.find(attrs={"data-stat": "height"}).get_text()
            weight = row.find(attrs={"data-stat": "weight"}).get_text()
            birth_date = row.find(attrs={"data-stat": "birth_date"}).get_text()
            # birth_date = row.find(attrs={"data-stat": "birth_date"})['csk']
            college = row.find(attrs={"data-stat": "colleges"}).get_text()
            if birth_date != '':
                bday = datetime.strptime(birth_date, '%B %d, %Y')
            else:
                bday = None
            print(name, playerid, link, year_max, height, weight, bday, college)

            cursor.execute("select * from Players where Link = ?", playerid)
            row = cursor.fetchone()
            if not row:
                cursor.execute("insert into Players(PlayerId, Name, Link, FirstYear, FinalYear, Height, Weight,
                                Birthday, College)"
                               "Values (?, ?, ?, ?, ?, ?, ?, ?, ?)"
                                , playerid, name, link, year_min, year_max, height, weight, bday, college)

    cursor.commit()

# Loop from a to z
# for i in range(ord('a'),ord('z') + 1):
#     get_players(chr(i))

def scrape_player_info(cursor, player, playerlink):
    url = 'https://www.basketball-reference.com' + playerlink
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    div = soup.find(id="info")
    if div.div.div.img is None:
        imgUrl = None
    else:
        imgUrl = div.div.div.img['src']
    # print(div)
    birthplace = div.find("span", itemprop="birthPlace").get_text().strip()
    jerseys = div.find("div", class_="uni_holder").find_all('a')
    jersey_no = None
    for jersey in jerseys:
        jersey_no = jersey.svg.get_text()
    print(player, jersey_no, imgUrl, birthplace)
    cursor.execute("update Players set Image = ?, Country = ?, JerseyNumber = ? where PlayerId = ?",
                   imgUrl, birthplace, jersey_no, player)
    cursor.commit()

def get_all_player():
    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()
    cursor.execute("select PlayerId, Link from Players")
    rows = cursor.fetchall()
    for row in rows:
        scrape_player_info(cursor, row.PlayerId, row.Link)

# get_all_player()

# DRAFT ----------------------------------------------------------------------------------------------------------------
def get_draft(cursor, year):

    url = "https://www.basketball-reference.com/draft/NBA_" + str(year) + ".html"
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="div_stats")
    # print(table)
    rows = table.tbody.find_all("tr")
    for row in rows:
        rank = row.find("th").get_text()
        if rank.isnumeric():
            player = row.find("td", attrs={"data-stat": "player"})
            if player.a is not None:
                team = row.find("td", attrs={"data-stat": "team_id"}).a['title']
                playerid = row.find("td", attrs={"data-stat": "player"}).a['href'][11:][:-5]
                print(rank, team, playerid)
                cursor.execute("update Players set DraftTeam = ?, DraftPick = ?, DraftYear = ? where PlayerId = ?",
                               team, rank, year, playerid)
    cursor.commit()


cnxn = pyodbc.connect(sql_connection)
cursor = cnxn.cursor()

for year in range(1950, 2021):
    get_draft(cursor, year)


# SALARY ---------------------------------------------------------------------------------------------------------------
def getSalaryInfo():
    def getSalaryforYear(row, year):
        data = row.find("td", attrs={"data-stat": year})
        if data.get_text() != '':
            return data['csk']
        else:
            return None

    cnxn = pyodbc.connect(sql_connection)
    cursor = cnxn.cursor()
    url = 'https://www.basketball-reference.com/contracts/players.html'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="player-contracts")
    rows = table.tbody.find_all("tr")
    for row in rows:
        rank = row.find("th").get_text()
        if rank.isnumeric():
            playerid = row.find("td", attrs={"data-stat": "player"})['data-append-csv']
            y1 = getSalaryforYear(row, 'y1')
            y2 = getSalaryforYear(row, 'y2')
            y3 = getSalaryforYear(row, 'y3')
            y4 = getSalaryforYear(row, 'y4')
            y5 = getSalaryforYear(row, 'y5')
            y6 = getSalaryforYear(row, 'y6')
            print(rank, playerid, y1, y2, y3, y4, y5, y6)
            cursor.execute("update Players set Salary1 = ?, Salary2 = ?, Salary3 = ?, Salary4 = ?,"
                           " Salary5 = ?, Salary6 = ? where PlayerId = ?",
                            y1, y2, y3, y4, y5, y6, playerid)

    cursor.commit()

# TEAMS
def team():
    url = 'https://www.basketball-reference.com/teams/'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    # print(soup)
    table = soup.find(id="teams_active")
    # print(table)

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    teams = table.find_all("tr", class_ = "full_table")
    for team in teams:
        team_id = team.th.a["href"]
        team_name = team.th.a.get_text()
        print(team_id[7:-1], team_name)
        cursor.execute("insert into Teams(TeamId, TeamName) values(?, ?)", team_id[7:-1], team_name)
    cursor.commit()

team()

# TEAM SALARY
def team_salary():
    def getSalary(row, year):
        salary = row.find("td", attrs={"data-stat": year})
        if salary is not None and salary.has_attr('csk'):
            return salary["csk"]
        else:
            return None

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    cursor.execute("select AltTeamId from Teams")
    teams = cursor.fetchall()

    for team in teams:
        url = 'https://www.basketball-reference.com/contracts/' + team.AltTeamId + '.html'
        page = requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')
        # print(soup)
        table = soup.find(id="contracts")
        print(url)
        # print(table)
        rows = table.find_all("tr")

        cnxn = pyodbc.connect(sql_connection)
        cursor = cnxn.cursor()

        for row in rows:
            player = row.find("th", attrs={"data-stat": "player", "scope": "row"})
            if player is not None and row.th.has_attr('csk'):
                # print(row)
                player_id = row.th['csk']
                y1 = getSalary(row, "y1")
                y2 = getSalary(row, "y2")
                y3 = getSalary(row, "y3")
                y4 = getSalary(row, "y4")
                y5 = getSalary(row, "y5")
                y6 = getSalary(row, "y6")
                print(player_id, y1, y2, y3, y4, y5, y6)
                cursor.execute("Insert into TeamSalary(TeamId, PlayerId, Salary1, Salary2, Salary3, Salary4, Salary5, "
                               "Salary6) Values(?, ?, ?, ?, ?, ?, ?, ?)",
                               team.AltTeamId, player_id, y1, y2, y3, y4, y5, y6)
        cursor.commit()

team_salary()

# TEAM LOGO
def team_logo():

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    cursor.execute("select AltTeamId from Teams")
    rows = cursor.fetchall()

    for row in rows:
        team_image = 'https://d2p3bygnnzw9w3.cloudfront.net/req/202107263/tlogo/bbr/' + row.AltTeamId + '-2021.png'

        cursor.execute("update Teams set TeamLogo = ? where AltTeamId = ?", team_image, row.AltTeamId)
        print(team_image)
    cursor.commit()

team_logo()

# TOTAL STATS
def getfromtotal(table, playoffs, player_id):
    def getotal(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            # print(row)

            season = row.th.get_text()
            age = getotal(row, "age")
            team_id = getotal(row, "team_id")
            pos = getotal(row, "pos")
            games_played = getotal(row, "g")
            games_started = getotal(row, "gs")
            min_played = getotal(row, "mp")
            fg = getotal(row, "fg")
            fga = getotal(row, "fga")
            fg3 = getotal(row, "fg3")
            fg3a = getotal(row, "fg3a")
            fg2 = getotal(row, "fg2")
            fg2a = getotal(row, "fg2a")
            ft = getotal(row, "ft")
            fta = getotal(row, "fta")
            off_reb = getotal(row, "orb")
            def_reb = getotal(row, "drb")
            rebounds = getotal(row, "trb")
            assists = getotal(row, "ast")
            steals = getotal(row, "stl")
            blocks = getotal(row, "blk")
            turnovers = getotal(row, "tov")
            fouls = getotal(row, "pf")
            points = getotal(row, "pts")

            # print(season, age, team_id, pos, games_played, games_started, min_played, fg, fga, fg3,
            #       fg3a, fg2, fg2a, ft, fta, off_reb, def_reb, rebounds, assists, steals,
            #       blocks, turnovers, fouls, points)
            cursor.execute("insert into TotalStats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed, "
                            "GamesStarted, MinutesPlayed, FgMade, FgAttempts, Fg3Made, Fg3Attempts, Fg2Made, "
                           "Fg2Attempts, FtMade, FtAttempts, OffRebound, DefRebound, Rebounds, Assists, Steals, "
                           "Blocks, Turnovers, Fouls, Points) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"
                           " ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", player_id, playoffs, season, age, team_id, pos,
                           games_played, games_started, min_played, fg, fga, fg3, fg3a, fg2, fg2a, ft, fta, off_reb,
                           def_reb, rebounds, assists, steals, blocks, turnovers, fouls, points)

    cursor.commit()

def scrape_total(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="totals")
    getfromtotal(table, 0, player_id)

    playoffs = soup.find(id="playoffs_totals")

    if table == None:
        return

    getfromtotal(playoffs, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_total(row.PlayerId, row.Link)

# ADVANCED STATS
def getfromadv(table, playoffs, player_id):
    def getadv(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            season = row.th.get_text()
            age = getadv(row, "age")
            team_id = getadv(row, "team_id")
            pos = getadv(row, "pos")
            per = getadv(row, "per")
            ts_pct = getadv(row, "ts_pct")
            fg3_rate = getadv(row, "fg3a_per_fga_pct")
            ft_rate = getadv(row, "fta_per_fga_pct")
            orb_pct = getadv(row, "orb_pct")
            drb_pct = getadv(row, "drb_pct")
            trb_pct = getadv(row, "trb_pct")
            ast_pct = getadv(row, "ast_pct")
            stl_pct = getadv(row, "stl_pct")
            blk_pct = getadv(row, "blk_pct")
            tov_pct = getadv(row, "tov_pct")
            usg_pct = getadv(row, "usg_pct")
            win_share = getadv(row, "ws")
            plus_minus = getadv(row, "bpm")
            obpm = getadv(row, "obpm")
            dbpm = getadv(row, "dbpm")

            # print(season, age, team_id, pos, per, ts_pct, fg3_rate, ft_rate, orb_pct, drb_pct, trb_pct, ast_pct,
            #       stl_pct, blk_pct, tov_pct, usg_pct, win_share, plus_minus)
            cursor.execute("insert into AdvancedStats (PlayerId, Playoffs, Season, Age, Team, Position, "
                           "PlayerEfficiency, TrueShooting, Fg3Rate, FtRate, OffRebRate, DefRebRate, ReboundRate, "
                           "AssistRate, StealRate, BlockRate, TurnoverRate, UsageRate, WinShares, PlusMinus, "
                           "OffPlusMinus, DefPlusMinus) "
                           "values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                           player_id, playoffs, season, age, team_id, pos, per, ts_pct, fg3_rate, ft_rate, orb_pct,
                           drb_pct, trb_pct, ast_pct, stl_pct, blk_pct, tov_pct, usg_pct, win_share, plus_minus,
                           obpm, dbpm)
    cursor.commit()

def scrape_adv(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="advanced")
    getfromadv(table, 0, player_id)

    playoffs = soup.find(id="playoffs_advanced")

    if table == None:
        return

    getfromadv(playoffs, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_adv(row.PlayerId, row.Link)

# PER 100 POS
def getfromper100(table, playoffs, player_id):
    def getper100(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            season = row.th.get_text()
            age = getper100(row, "age")
            team_id = getper100(row, "team_id")
            pos = getper100(row, "pos")
            games_played = getper100(row, "g")
            games_started = getper100(row, "gs")
            fg = getper100(row, "fg_per_poss")
            fga = getper100(row, "fga_per_poss")
            fg3 = getper100(row, "fg3_per_poss")
            fg3a = getper100(row, "fg3a_per_poss")
            fg2 = getper100(row, "fg2_per_poss")
            fg2a = getper100(row, "fg2a_per_poss")
            ft = getper100(row, "ft_per_poss")
            fta = getper100(row, "fta_per_poss")
            off_reb = getper100(row, "orb_per_poss")
            def_reb = getper100(row, "drb_per_poss")
            rebounds = getper100(row, "trb_per_poss")
            assists = getper100(row, "ast_per_poss")
            steals = getper100(row, "stl_per_poss")
            blocks = getper100(row, "blk_per_poss")
            turnovers = getper100(row, "tov_per_poss")
            fouls = getper100(row, "pf_per_poss")
            points = getper100(row, "pts_per_poss")
            off_rating = getper100(row, "off_rtg")
            def_rating = getper100(row, "def_rtg")

            # print(season, age, team_id, pos, games_played, games_started, fg, fga, fg3, fg3a, fg2, fg2a, ft, fta,
            #       off_reb, def_reb, rebounds, assists, steals, blocks, turnovers, fouls, points, off_rating,
            #       def_rating)

            cursor.execute("insert into Per100Stats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed, "
                           "GamesStarted, FgMade, FgAttempts, Fg3Made, Fg3Attempts, Fg2Made, Fg2Attempts, FtMade, "
                           "FtAttempts, OffRebound, DefRebound, Rebounds, Assists, Steals, Blocks, Turnovers, Fouls, "
                           "Points, OffRating, DefRating) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"
                           " ?, ?, ?, ?, ?, ?, ?, ?, ?)", player_id, playoffs, season, age, team_id, pos, games_played,
                           games_started, fg, fga, fg3, fg3a, fg2, fg2a, ft, fta, off_reb, def_reb, rebounds, assists,
                           steals, blocks, turnovers, fouls, points, off_rating, def_rating)
        cursor.commit()

def scrape_adv(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="per_poss")
    if table is None:
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in comments:
            comment_soup = BeautifulSoup(comment, 'html.parser')
            table = comment_soup.find(id="per_poss")
            if table is not None:
                break

    getfromper100(table, 0, player_id)

    playoffs = soup.find(id="playoffs_per_poss")
    if playoffs is None:
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in comments:
            comment_soup = BeautifulSoup(comment, 'html.parser')
            playoffs = comment_soup.find(id="playoffs_per_poss")
            if playoffs is not None:
                break

    if playoffs == None:
        return

    getfromper100(playoffs, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_adv(row.PlayerId, row.Link)

# PER 36 MIN
def getfromper36(table, playoffs, player_id):
    def getper36(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            season = row.th.get_text()
            age = getper36(row, "age")
            team_id = getper36(row, "team_id")
            pos = getper36(row, "pos")
            games_played = getper36(row, "g")
            games_started = getper36(row, "gs")
            fg = getper36(row, "fg_per_mp")
            fga = getper36(row, "fga_per_mp")
            fg3 = getper36(row, "fg3_per_mp")
            fg3a = getper36(row, "fg3a_per_mp")
            fg2 = getper36(row, "fg2_per_mp")
            fg2a = getper36(row, "fg2a_per_mp")
            ft = getper36(row, "ft_per_mp")
            fta = getper36(row, "fta_per_mp")
            off_reb = getper36(row, "orb_per_mp")
            def_reb = getper36(row, "drb_per_mp")
            rebounds = getper36(row, "trb_per_mp")
            assists = getper36(row, "ast_per_mp")
            steals = getper36(row, "stl_per_mp")
            blocks = getper36(row, "blk_per_mp")
            turnovers = getper36(row, "tov_per_mp")
            fouls = getper36(row, "pf_per_mp")
            points = getper36(row, "pts_per_mp")

            cursor.execute("insert into Per36Stats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed, "
                           "GamesStarted, Fg, Fga, Fg3, Fg3a, Fg2, Fg2a, Ft, "
                           "Fta, OffRebound, DefRebound, Rebounds, Assists, Steals, Blocks, Turnovers, Fouls, "
                           "Points) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"
                           " ?, ?, ?, ?, ?, ?, ?)", player_id, playoffs, season, age, team_id, pos, games_played,
                           games_started, fg, fga, fg3, fg3a, fg2, fg2a, ft, fta, off_reb, def_reb, rebounds, assists,
                           steals, blocks, turnovers, fouls, points)
        cursor.commit()

def scrape_adv(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="per_minute")
    if table is None:
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in comments:
            comment_soup = BeautifulSoup(comment, 'html.parser')
            table = comment_soup.find(id="per_minute")
            if table is not None:
                break

    getfromper36(table, 0, player_id)

    playoffs = soup.find(id="playoffs_per_minute")
    if playoffs is None:
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in comments:
            comment_soup = BeautifulSoup(comment, 'html.parser')
            playoffs = comment_soup.find(id="playoffs_per_minute")
            if playoffs is not None:
                break

    if playoffs == None:
        return

    getfromper36(playoffs, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_adv(row.PlayerId, row.Link)
