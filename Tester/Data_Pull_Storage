# STATS ----------------------------------------------------------------------------------------------------------------
import pyodbc
import requests
from bs4 import BeautifulSoup
sql_connection = 'DRIVER={SQL Server Native Client 11.0};SERVER=localhost;DATABASE=BotTester;UID=sa;PWD=wa11paper'

def getfromtable(table, playoffs, player_id):
    def getstat(row, tag):
        data = row.find(attrs={"data-stat": tag})
        if data is not None:
            return data.get_text()
        else:
            return None

    if table is None:
        return
    rows = table.find_all("tr")
    for row in rows:
        if (row.th != None and row.th['data-stat'] == 'season' and row.th['scope'] == 'row' and row.th.get_text()!= ""):
            # print(row)
            season = row.th.get_text()
            age = getstat(row, "age")
            team_id = getstat(row, "team_id")
            pos = getstat(row, "pos")
            g = getstat(row, "g")
            gs = getstat(row, "gs")
            mp_per_g = getstat(row, "mp_per_g")
            fg_per_g = getstat(row, "fg_per_g")
            fga_per_g = getstat(row, "fga_per_g")
            fg_pct = getstat(row, "fg_pct")
            fg3_per_g = getstat(row, "fg3_per_g")
            fg3a_per_g = getstat(row, "fg3a_per_g")
            fg3_pct = getstat(row, "fg3_pct")
            fg2_per_g = getstat(row, "fg2_per_g")
            fg2a_per_g = getstat(row, "fg2a_per_g")
            fg2_pct = getstat(row, "fg2_pct")
            efg_pct = getstat(row, "efg_pct")
            ft_per_g = getstat(row, "ft_per_g")
            fta_per_g = getstat(row, "fta_per_g")
            ft_pct = getstat(row, "ft_pct")
            orb_per_g = getstat(row, "orb_per_g")
            drb_per_g = getstat(row, "drb_per_g")
            trb_per_g = getstat(row, "trb_per_g")
            ast_per_g = getstat(row, "ast_per_g")
            stl_per_g = getstat(row, "stl_per_g")
            blk_per_g = getstat(row, "blk_per_g")
            tov_per_g = getstat(row, "tov_per_g")
            pf_per_g = getstat(row, "pf_per_g")
            pts_per_g = getstat(row, "pts_per_g")

            # print(season, age, team_id, pos, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per_g, fg3a_per_g,
            #       fg3_pct, fg2_per_g, fg2a_per_g, fg2_pct, efg_pct, ft_per_g, fta_per_g, ft_pct, orb_per_g, drb_per_g,
            #       trb_per_g, ast_per_g, stl_per_g, blk_per_g, tov_per_g, pf_per_g, pts_per_g)

            # cursor.execute("insert into PlayerStats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed"
            #                ")"
            #                "values (?, ?, ?, ?, ?, ?, ?)",
            #                player_id, playoffs, season, age, team_id, pos, g)
            cursor.execute("insert into PlayerStats (PlayerId, Playoffs, Season, Age, Team, Position, GamesPlayed,"
                           " GamesStarted, MinPerGame, FgPerGame, FgaPerGame, FgPercent, Fg3PerGame, Fg3aPerGame, "
                           "Fg3Percent, Fg2PerGame, Fg2aPerGame, Fg2Percent, EfgPercent, FtPerGame, FtaPerGame, "
                           "FtPercent, OffRebound, DefRebound, Rebounds, Assists, Steals, Blocks, Turnovers, Fouls, "
                           "Points) "
                           "values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,"
                           " ?, ?, ?)",
                           player_id, playoffs, season, age, team_id, pos, g, gs, mp_per_g, fg_per_g,
                           fga_per_g, fg_pct, fg3_per_g, fg3a_per_g, fg3_pct, fg2_per_g, fg2a_per_g, fg2_pct, efg_pct,
                           ft_per_g, fta_per_g, ft_pct, orb_per_g, drb_per_g, trb_per_g, ast_per_g, stl_per_g,
                           blk_per_g, tov_per_g, pf_per_g, pts_per_g)

    cursor.commit()

def scrape_data(player_id, link):

    url = 'https://www.basketball-reference.com' + link
    print(url)
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="per_game")
    getfromtable(table, 0, player_id)

    table = soup.find(id="playoffs_per_game")

    if table == None:
        return

    getfromtable(table, 1, player_id)

cnxn = pyodbc.connect(sql_connection)

cursor = cnxn.cursor()

# cursor.execute("select top 2 PlayerId, Link from Players where playerid = 'architi01'")
cursor.execute("select PlayerId, Link from Players")
rows = cursor.fetchall()
for row in rows:
    print(row.PlayerId)
    scrape_data(row.PlayerId, row.Link)

# PLAYERINFO -----------------------------------------------------------------------------------------------------------
def get_players(str):
    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    url = 'https://www.basketball-reference.com/players/' + str

    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="div_players")
    rows = table.find_all("tr")
    for row in rows:
        if row.th is not None and row.th['data-stat'] == 'player' \
                and row.th['scope'] == 'row' and row.th.get_text() != "":
            # print(row)
            name = row.th.get_text()
            playerid = row.th['data-append-csv']
            link = row.th.a['href']
            year_min = row.find(attrs={"data-stat": "year_min"}).get_text()
            year_max = row.find(attrs={"data-stat": "year_max"}).get_text()
            height = row.find(attrs={"data-stat": "height"}).get_text()
            weight = row.find(attrs={"data-stat": "weight"}).get_text()
            birth_date = row.find(attrs={"data-stat": "birth_date"}).get_text()
            # birth_date = row.find(attrs={"data-stat": "birth_date"})['csk']
            college = row.find(attrs={"data-stat": "colleges"}).get_text()
            if birth_date != '':
                bday = datetime.strptime(birth_date, '%B %d, %Y')
            else:
                bday = None
            print(name, playerid, link, year_max, height, weight, bday, college)

            cursor.execute("select * from Players where Link = ?", playerid)
            row = cursor.fetchone()
            if not row:
                cursor.execute("insert into Players(PlayerId, Name, Link, FirstYear, FinalYear, Height, Weight,
                                Birthday, College)"
                               "Values (?, ?, ?, ?, ?, ?, ?, ?, ?)"
                                , playerid, name, link, year_min, year_max, height, weight, bday, college)

    cursor.commit()

# Loop from a to z
# for i in range(ord('a'),ord('z') + 1):
#     get_players(chr(i))

def scrape_player_info(cursor, player, playerlink):
    url = 'https://www.basketball-reference.com' + playerlink
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    div = soup.find(id="info")
    if div.div.div.img is None:
        imgUrl = None
    else:
        imgUrl = div.div.div.img['src']
    # print(div)
    birthplace = div.find("span", itemprop="birthPlace").get_text().strip()
    jerseys = div.find("div", class_="uni_holder").find_all('a')
    jersey_no = None
    for jersey in jerseys:
        jersey_no = jersey.svg.get_text()
    print(player, jersey_no, imgUrl, birthplace)
    cursor.execute("update Players set Image = ?, Country = ?, JerseyNumber = ? where PlayerId = ?",
                   imgUrl, birthplace, jersey_no, player)
    cursor.commit()

def get_all_player():
    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()
    cursor.execute("select PlayerId, Link from Players")
    rows = cursor.fetchall()
    for row in rows:
        scrape_player_info(cursor, row.PlayerId, row.Link)

# get_all_player()

# DRAFT ----------------------------------------------------------------------------------------------------------------
def get_draft(cursor, year):

    url = "https://www.basketball-reference.com/draft/NBA_" + str(year) + ".html"
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="div_stats")
    # print(table)
    rows = table.tbody.find_all("tr")
    for row in rows:
        rank = row.find("th").get_text()
        if rank.isnumeric():
            player = row.find("td", attrs={"data-stat": "player"})
            if player.a is not None:
                team = row.find("td", attrs={"data-stat": "team_id"}).a['title']
                playerid = row.find("td", attrs={"data-stat": "player"}).a['href'][11:][:-5]
                print(rank, team, playerid)
                cursor.execute("update Players set DraftTeam = ?, DraftPick = ?, DraftYear = ? where PlayerId = ?",
                               team, rank, year, playerid)
    cursor.commit()


cnxn = pyodbc.connect(sql_connection)
cursor = cnxn.cursor()

for year in range(1950, 2021):
    get_draft(cursor, year)


# SALARY ---------------------------------------------------------------------------------------------------------------
def getSalaryInfo():
    def getSalaryforYear(row, year):
        data = row.find("td", attrs={"data-stat": year})
        if data.get_text() != '':
            return data['csk']
        else:
            return None

    cnxn = pyodbc.connect(sql_connection)
    cursor = cnxn.cursor()
    url = 'https://www.basketball-reference.com/contracts/players.html'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find(id="player-contracts")
    rows = table.tbody.find_all("tr")
    for row in rows:
        rank = row.find("th").get_text()
        if rank.isnumeric():
            playerid = row.find("td", attrs={"data-stat": "player"})['data-append-csv']
            y1 = getSalaryforYear(row, 'y1')
            y2 = getSalaryforYear(row, 'y2')
            y3 = getSalaryforYear(row, 'y3')
            y4 = getSalaryforYear(row, 'y4')
            y5 = getSalaryforYear(row, 'y5')
            y6 = getSalaryforYear(row, 'y6')
            print(rank, playerid, y1, y2, y3, y4, y5, y6)
            cursor.execute("update Players set Salary1 = ?, Salary2 = ?, Salary3 = ?, Salary4 = ?,"
                           " Salary5 = ?, Salary6 = ? where PlayerId = ?",
                            y1, y2, y3, y4, y5, y6, playerid)

    cursor.commit()

# TEAMS
def team():
    url = 'https://www.basketball-reference.com/teams/'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    # print(soup)
    table = soup.find(id="teams_active")
    # print(table)

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    teams = table.find_all("tr", class_ = "full_table")
    for team in teams:
        team_id = team.th.a["href"]
        team_name = team.th.a.get_text()
        print(team_id[7:-1], team_name)
        cursor.execute("insert into Teams(TeamId, TeamName) values(?, ?)", team_id[7:-1], team_name)
    cursor.commit()

team()

# TEAM SALARY
def team_salary():
    def getSalary(row, year):
        salary = row.find("td", attrs={"data-stat": year})
        if salary is not None and salary.has_attr('csk'):
            return salary["csk"]
        else:
            return None

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    cursor.execute("select AltTeamId from Teams")
    teams = cursor.fetchall()

    for team in teams:
        url = 'https://www.basketball-reference.com/contracts/' + team.AltTeamId + '.html'
        page = requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')
        # print(soup)
        table = soup.find(id="contracts")
        print(url)
        # print(table)
        rows = table.find_all("tr")

        cnxn = pyodbc.connect(sql_connection)
        cursor = cnxn.cursor()

        for row in rows:
            player = row.find("th", attrs={"data-stat": "player", "scope": "row"})
            if player is not None and row.th.has_attr('csk'):
                # print(row)
                player_id = row.th['csk']
                y1 = getSalary(row, "y1")
                y2 = getSalary(row, "y2")
                y3 = getSalary(row, "y3")
                y4 = getSalary(row, "y4")
                y5 = getSalary(row, "y5")
                y6 = getSalary(row, "y6")
                print(player_id, y1, y2, y3, y4, y5, y6)
                cursor.execute("Insert into TeamSalary(TeamId, PlayerId, Salary1, Salary2, Salary3, Salary4, Salary5, "
                               "Salary6) Values(?, ?, ?, ?, ?, ?, ?, ?)",
                               team.AltTeamId, player_id, y1, y2, y3, y4, y5, y6)
        cursor.commit()

team_salary()

# TEAM LOGO
def team_logo():

    cnxn = pyodbc.connect(sql_connection)

    cursor = cnxn.cursor()

    cursor.execute("select AltTeamId from Teams")
    rows = cursor.fetchall()

    for row in rows:
        team_image = 'https://d2p3bygnnzw9w3.cloudfront.net/req/202107263/tlogo/bbr/' + row.AltTeamId + '-2021.png'

        cursor.execute("update Teams set TeamLogo = ? where AltTeamId = ?", team_image, row.AltTeamId)
        print(team_image)
    cursor.commit()

team_logo()